{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850665d-debc-4cc0-90ae-91ce5742a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "# stop word removal, word tokenization, sentence tokenization, uppercase lowercase text, \n",
    "# pattern matching (date email etc), dirty text cleaning (removal of special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caae810-a1a3-4725-8167-9ac931e140b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Collecting regex>=2021.8.3\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f86d18ad420>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /packages/81/8a/96a62ce98e8ff1b16db56fde3debc8a571f6b7ea42ee137eb0d995cdfa26/regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 KB\u001b[0m \u001b[31m612.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m496.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, tqdm, threadpoolctl, regex, numpy, joblib, scipy, pandas, nltk, scikit-learn\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script f2py is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed joblib-1.3.2 nltk-3.8.1 numpy-1.26.3 pandas-2.2.0 regex-2023.12.25 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0 tqdm-4.66.1 tzdata-2023.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f406778e-690c-4321-9b48-3e6d4cccb566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/student/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/student/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/student/.local/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/student/.local/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92309d4-da12-496c-8a08-a7e472d269ca",
   "metadata": {},
   "source": [
    "***Word tokenization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fef0cc-6da6-4976-8063-4459f94dd1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# word_tokens = word_tokenize(text)\n",
    "\n",
    "filtered_sentence = []\n",
    "for word in text.split(' '):\n",
    "    filtered_sentence.append(word)\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925037f2-a61a-4bf2-8f60-2f11b2f59445",
   "metadata": {},
   "source": [
    "***Sentence tokenization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c7a67b4-e934-46d0-b069-037310e25d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a simple sentence to check the sentence tokenization', ' Zeeshan sir told us to do it for our NLP lab', ' This system runs on Ubuntu', '']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"This is a simple sentence to check the sentence tokenization. Zeeshan sir told us to do it for our NLP lab. This program runs on Mac.\"\n",
    "\n",
    "sentence_tokens = []\n",
    "\n",
    "for word in text2.split('.'):\n",
    "    sentence_tokens.append(word)\n",
    "\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb6a20-8fdd-44ee-8f67-b3464e881898",
   "metadata": {},
   "source": [
    "***Letter uppercase lowercase conversion***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1db39dab-1673-4e1c-bf14-71f263460534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS A SIMPLE TEXT TO SHOW THAT 56 > 34\n",
      "this is a simple text to show that 56 > 34\n"
     ]
    }
   ],
   "source": [
    "text3 = \"this is a simple text to show that 56 > 34\"\n",
    "\n",
    "text3 = text3.upper()\n",
    "print(text3)\n",
    "\n",
    "text3 = text3.lower()\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e06784-808c-4c8f-b0b5-387b614097f0",
   "metadata": {},
   "source": [
    "***Stop word removal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b02e1f3-3535-48f2-8514-918ded374f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stopwords_removed = []\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    if word not in stop_words:\n",
    "        stopwords_removed.append(word)\n",
    "\n",
    "print(stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb371d6f-3751-47b0-a6c1-5021761bc7a3",
   "metadata": {},
   "source": [
    "***Stemming and Lemmatization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0637759e-0e2c-4cca-bbe7-93dcc2b32cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking: walk\n",
      "sang: sang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/student/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "ps = PorterStemmer()\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "text5 = \"walking\"\n",
    "text6 = \"sang\"\n",
    "\n",
    "print(text5 + \": \" + ps.stem(text5))\n",
    "print(text6 + \": \"  + wl.lemmatize(text6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bae010-5c41-4016-a492-600b988e294b",
   "metadata": {},
   "source": [
    "***Remove punctuation and special characters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08960aaf-19fa-42a4-8ed2-844f18ac1de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is text to show special maybe they are special i dont know exactly why they are called special characters     \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text4 = \"This is text to show special (maybe, they are special; i don't know exactly why they are called special) characters {} [] | + $$$.\"\n",
    "pattern = \"[^A-Za-z0-9 ]+\"\n",
    "\n",
    "res = re.sub(pattern, '', text4)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d27517-97d2-43b6-922f-9af0adb89ffd",
   "metadata": {},
   "source": [
    "***Remove URLs and HTML tags***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "849f54ce-99c7-4481-9138-7714b7a1028b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text with the URL \n"
     ]
    }
   ],
   "source": [
    "text5 = \"\"\"<html>This is a sample text with the URL https://www.google.com<a href=\"https://some-url-here\"></a></html>\"\"\"\n",
    "p = re.compile(r'<.*?>')\n",
    "ans = p.sub('', text5)\n",
    "ans = re.sub(r'https?://\\S+', '', ans)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075495ff-5a3c-4085-a02f-a946fc02caa2",
   "metadata": {},
   "source": [
    "***Replace contractions with their explanations (ex: can't -> cannot)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11c3d0-2c36-4df8-95d6-0c735f74f060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7717a892-93e5-4147-9d9c-0d5979a4a0fc",
   "metadata": {},
   "source": [
    "***Convert date and currency into single standard format***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61b299-4949-47e3-80e9-1b6dddc97c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
